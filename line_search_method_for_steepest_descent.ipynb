{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2bb940b",
   "metadata": {},
   "source": [
    "### Line Search Method for Steepest Descent\n",
    "\n",
    "Overall Goal - to solve minimization problem $: \\min_{x \\in R^n} f(x), x^* - $ solution.\n",
    "\n",
    "The most common practice is iteratively look for $x^{k+1}: f(x^{k+1}) < f(x^{k})$, so called descent method.\n",
    "\n",
    "Eventually, outcome of that search would be \"$x^{k+1}$ sufficiently close to $x^*$\".\n",
    "\n",
    "Okay, we have general framework, but how to find that $x^{k+1}$? Previosly I have mentioned that such framework called \"descent\". So, as descending we can choose the \"direction\" ($p^k$) and the \"length\" ($\\alpha^k$) of the step $: x^{k+1} := x^k + \\alpha^k p^k$.\n",
    "\n",
    "The most straight-forward choice of search direction is $-\\nabla f(x^k)$ as opposite to the direction of greatest growth, is called steepest-descent method. Such choice have some strong advantage like global convergency but in reality numerically it is often not convergent at all. \n",
    "\n",
    "Nevertheless, we obtain the following expression $: f(x^k - \\alpha^k  \\nabla f(x^k)) < f(x^k)$. The last question: which step length $\\alpha$ to choose? In old days, guys would solve exact problem to pick good $\\alpha^k: \\min_{\\alpha} f(x^k - \\alpha \\nabla f(x_k)), \\alpha \\geq 0$. But this method is not considered cost effective. So, let's just pick randomly some $\\alpha$ with hope to get some \"good\" step length just to demonstrate general method. Later, I will show more optimal startegy to choose $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29d792e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.99999317,  1.49998895]), np.float64(-1.2499999999355231))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "class SteepestDescent:\n",
    "    def __init__(self, alpha: float, epsilon: float = 1e-5, max_iter: int = 10**7):\n",
    "        if alpha < 0 or not isinstance(alpha, (int, float)):\n",
    "            raise ValueError(\"'alpha' must be non-negative number.\")\n",
    "        self.alpha = alpha\n",
    "\n",
    "        if epsilon <= 0 or not isinstance(max_iter, (int, float)):\n",
    "            raise ValueError(\"'epsilon' must be small positive number.\")\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "        if max_iter <= 0 or not isinstance(max_iter, int):\n",
    "            raise ValueError(\"'max_iter' must be positive int number.\")\n",
    "        self._max_iter = max_iter\n",
    "\n",
    "    def solve(self, grad_f: Callable, x0: NDArray) -> NDArray:\n",
    "        \"\"\" \n",
    "        The method to find solution of minimizing function 'f' with gradient 'grad_f' and initial guess 'x0'.\n",
    "        \"\"\"\n",
    "        xk = x0.copy()\n",
    "        for _ in range(0, self._max_iter+1):\n",
    "            grad_f_xk = grad_f(xk)\n",
    "            if np.linalg.norm(grad_f_xk) <= self._epsilon:\n",
    "                break\n",
    "            xk -= self.alpha * grad_f_xk\n",
    "        else:\n",
    "            warnings.warn(\"You have reached the max iteration limit.\")\n",
    "        return xk\n",
    "        \n",
    "\n",
    "f = lambda x: x[0] - x[1] + 2*x[0]*x[1] + 2*x[0]*x[0] + x[1]*x[1]\n",
    "gf = lambda x: np.array([1+2*x[1]+4*x[0],-1+2*x[0]+2*x[1]])\n",
    "\n",
    "w_hat = SteepestDescent(alpha=1e-2).solve(gf, np.ones(shape=2))\n",
    "w_hat, f(w_hat) # expect to obtain: x*=[-1,1.5] and f(x*)=-1.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d649574e",
   "metadata": {},
   "source": [
    "Let's check work of algorithm on more sofisticated case.\n",
    "\n",
    "$ f(x_1, x_2, x_3) = (x_1+2x_2+x_1x_3)^2 + 2 (x_3 + x_2^2)^2 + c x_1^4$\n",
    "\n",
    "$\\nabla f = \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\frac{\\partial f}{\\partial x_3}\\right)$\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial x_1}  = 2(x_1+2x_2+x_1x_3) (x_3 + 1) + 4c x_1^3 $\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial x_2}  = 4(x_1+2x_2+x_1x_3) + 8 x_2 (x_3 + x_2^2) $\n",
    "\n",
    "$ \\frac{\\partial f}{\\partial x_3}  = 2 x_1 (x_1+2x_2+x_1x_3) + 4 (x_3 + x_2^2)$\n",
    "\n",
    "Theoretical solution for different c: $x^* = (0,0,0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "019090ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=1\n",
      "x*=[ 1.40862400e-02 -7.04332962e-03 -4.96163716e-05]\n",
      "f(x*)=3.9372602574777596e-08\n",
      "f(x*_theory)=0\n",
      "==================================================\n",
      "c=10\n",
      "x*=[ 6.53829146e-03 -3.26967001e-03 -1.06944045e-05]\n",
      "f(x*)=1.827623926165688e-08\n",
      "f(x*_theory)=0\n",
      "==================================================\n",
      "c=100\n",
      "x*=[-3.03481009e-03  1.51796106e-03 -2.30591004e-06]\n",
      "f(x*)=8.483795383862314e-09\n",
      "f(x*_theory)=0\n",
      "==================================================\n",
      "Time to find solutions: 18.460739300004207s\n"
     ]
    }
   ],
   "source": [
    "f_xc = lambda x, c: (x[0]+2*x[1]+x[0]*x[2])*(x[0]+2*x[1]+x[0]*x[2]) + 2 * (x[2]+x[1]*x[1]) * (x[2]+x[1]*x[1]) + c * x[0] * x[0] * x[0] * x[0]\n",
    "grad_f_xc = lambda x, c: np.array([\n",
    "    2 * (x[0] + 2*x[1] + x[0]*x[2]) * (x[2] + 1) + 4 * c * x[0] * x[0] * x[0],\n",
    "    4 * (x[0] + 2*x[1] + x[0]*x[2]) + 8 * x[1] * (x[2] + x[1] * x[1]),\n",
    "    2 * x[0] * (x[0] + 2*x[1] + x[0]*x[2]) + 4 * (x[2] + x[1] * x[1])\n",
    "])\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "t0 = perf_counter()\n",
    "for c in [1, 10, 100]:\n",
    "    f_x = lambda x: f_xc(x, c)\n",
    "    grad_f_x = lambda x: grad_f_xc(x, c)\n",
    "    x0 = np.ones(shape=3)\n",
    "    w_hat = SteepestDescent(alpha=1e-2/c).solve(grad_f_x, x0)\n",
    "\n",
    "    print(f'{c=}')\n",
    "    print(f'x*={w_hat}')\n",
    "    print(f'f(x*)={f_x(w_hat)}')\n",
    "    print(f'f(x*_theory)={f_x([0,0,0])}')\n",
    "    print(50*\"=\")\n",
    "print(f\"Time to find solutions: {(perf_counter()-t0)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada31596",
   "metadata": {},
   "source": [
    "Great. Algorithm works. But, as you can see, we fix different alphas for different cases: in some cases, 0.01 is small enough for fast convergence, but in other cases,  0.01 was too larger (when c=100, gradient became too large -> we need to choose a pretty small alpha value so as not to not jump over solution).\n",
    "\n",
    "To cope with such scenarios, Wolfe's conditions were proposed, which allow you to choose a specific step length so that it is not too long and not too short.\n",
    "\n",
    "Wolfe Conditions consist of two conditions:\n",
    "- Armijo (Sufficient Decrease) Condition: $f(x^k + \\alpha p^k)) \\leq f(x^k) + c_1 \\alpha^k {p^k}^T \\nabla f(x^k), p^k = -\\nabla f(x^k), c_1 \\in (0,1), $ often set as $0.001$. This condition ensures the computed step length can sufficiently decrease the objective function $f(x^k)$.\n",
    "- Curvature Condition: $\\nabla f(x^k + \\alpha p^k)^T p^k \\geq c_2 \\nabla f(x^k)^T p^k, c_2 \\in (c_1,1), $ often set as $0.1$. This condition ensures a sufficient increase of the gradient.\n",
    "\n",
    "Therefore, under these conditions, we can sufficiently well find $\\alpha^k$ at each step by narrowing down the range of possible values that satisfy these conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52b9ee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=1\n",
      "x*=[-1.30705169e-02  6.53475355e-03 -4.27147047e-05]\n",
      "f(x*)=2.918596741688725e-08\n",
      "f(x*_theory)=0\n",
      "==================================================\n",
      "c=10\n",
      "x*=[ 6.00769694e-03 -3.00495991e-03 -9.02957268e-06]\n",
      "f(x*)=1.3031814943839697e-08\n",
      "f(x*_theory)=0\n",
      "==================================================\n",
      "c=100\n",
      "x*=[-2.78638646e-03  1.39432874e-03 -1.94405421e-06]\n",
      "f(x*)=6.033073327880847e-09\n",
      "f(x*_theory)=0\n",
      "==================================================\n",
      "Time to find solutions: 0.5274376000161283s\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "class SteepestDescentWeakWolfeConditions:\n",
    "    def __init__(self, epsilon: float = 1e-5, max_iter: int = 10**6):\n",
    "        self.init_alpha = 1.0\n",
    "\n",
    "        if epsilon <= 0 or not isinstance(max_iter, (int, float)):\n",
    "            raise ValueError(\"'epsilon' must be small positive number.\")\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "        if max_iter <= 0 or not isinstance(max_iter, int):\n",
    "            raise ValueError(\"'max_iter' must be positive int number.\")\n",
    "        self._max_iter = max_iter\n",
    "\n",
    "    def solve(self, f: Callable, grad_f: Callable, x0: NDArray) -> NDArray:\n",
    "        \"\"\" \n",
    "        The method to find solution of minimizing function 'f' with gradient 'grad_f' and initial guess 'x0'.\n",
    "        \"\"\"\n",
    "        xk = x0.copy()\n",
    "        for _ in range(0, self._max_iter+1):\n",
    "            grad_f_xk = grad_f(xk)\n",
    "            if np.linalg.norm(grad_f_xk) <= self._epsilon:\n",
    "                break\n",
    "            alpha = self._line_search_alpha_under_wolfe_conditions(f, grad_f, xk)\n",
    "            xk -= alpha * grad_f_xk\n",
    "        else:\n",
    "            warnings.warn(\"You have reached the max iteration limit.\")\n",
    "        return xk\n",
    "        \n",
    "    def _line_search_alpha_under_wolfe_conditions(self, f: Callable, grad_f: Callable, x, c1=1e-3, c2=0.1, max_iter=100):\n",
    "        alpha = self.init_alpha\n",
    "        alpha_min = 0\n",
    "        alpha_max = np.inf\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            \n",
    "            # ! Armijo Condition\n",
    "            if f(x - alpha * grad_f(x)) > f(x) - c1 * alpha * np.dot(-grad_f(x), grad_f(x)):\n",
    "                alpha_max = alpha\n",
    "                alpha = (alpha_min + alpha_max) / 2.0\n",
    "                continue\n",
    "            \n",
    "            # Curvature Condition\n",
    "            if np.dot(grad_f(x - alpha * grad_f(x)), -grad_f(x)) >= c2 * np.dot(grad_f(x), -grad_f(x)):\n",
    "                return alpha\n",
    "            \n",
    "            # -> alpha too small -> need to increase\n",
    "            \n",
    "            alpha_min = alpha\n",
    "            \n",
    "            if alpha_max == np.inf:\n",
    "                alpha = alpha * 2.0\n",
    "            else:\n",
    "                alpha = (alpha_min + alpha_max) / 2.0\n",
    "        else:\n",
    "            warnings.warn(\"You have reached the max iteration limit for alpha line search.\")\n",
    "        return alpha\n",
    "\n",
    "f_xc = lambda x, c: (x[0]+2*x[1]+x[0]*x[2])*(x[0]+2*x[1]+x[0]*x[2]) + 2 * (x[2]+x[1]*x[1]) * (x[2]+x[1]*x[1]) + c * x[0] * x[0] * x[0] * x[0]\n",
    "grad_f_xc = lambda x, c: np.array([\n",
    "    2 * (x[0] + 2*x[1] + x[0]*x[2]) * (x[2] + 1) + 4 * c * x[0] * x[0] * x[0],\n",
    "    4 * (x[0] + 2*x[1] + x[0]*x[2]) + 8 * x[1] * (x[2] + x[1] * x[1]),\n",
    "    2 * x[0] * (x[0] + 2*x[1] + x[0]*x[2]) + 4 * (x[2] + x[1] * x[1])\n",
    "])\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "t0 = perf_counter()\n",
    "for c in [1, 10, 100]:\n",
    "    f_x = lambda x: f_xc(x, c)\n",
    "    grad_f_x = lambda x: grad_f_xc(x, c)\n",
    "    x0 = np.ones(shape=3)\n",
    "    w_hat = SteepestDescentWeakWolfeConditions().solve(f_x, grad_f_x, x0)\n",
    "\n",
    "    print(f'{c=}')\n",
    "    print(f'x*={w_hat}')\n",
    "    print(f'f(x*)={f_x(w_hat)}')\n",
    "    print(f'f(x*_theory)={f_x([0,0,0])}')\n",
    "    print(50*\"=\")\n",
    "print(f\"Time to find solutions: {(perf_counter()-t0)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dacaba",
   "metadata": {},
   "source": [
    "Wooow!!!!! Realy good perfomance boost: 35x speedup under similar results just using wolfe conidtions instead of fixed alpha. Impressive.\n",
    "\n",
    "The most interesting thing is that this time we solved two optimization problems: external gradient descent and internal optimization of the descent step length parameter. Intuitively, it might seem that the nested optimization problem should, on the contrary, worsen performance, since each time, at each step, a new optimization problem of searching for alpha must be solved. However, if you think about it, these one-dimensional optimizations allowed us to SIGNIFICANTLY reduce the number of external “expensive” multidimensional optimization steps, which is how we achieved the benefit.\n",
    "\n",
    "However, as you may have noticed, I named the previous class “...WeakWolfeConditions,” and this is not without reason. The fact is that the above conditions can result in an $\\alpha$ value that is not close to the minimizer of $\\phi(\\alpha)$. The (weak) Wolfe conditions can be modified by using the following condition called Strong Wolfe condition, which writes the Curvature condition in absolute values:\n",
    "$$ p^k \\nabla f(x^k + \\alpha p^k) \\leq c_2 | {p^k}^T f(x^k)|, p^k = - \\nabla f(x^k) $$\n",
    "The Strong Wolfe Curvature condition restricts the slope of $\\phi(\\alpha)$ from getting too positive, hence excluding points far away from the stationary point of $\\phi$. In (weak) Wolfe, if the slope is positive ($>0$), we technically satisfy the condition and stop (or keep going). In Strong Wolfe, a positive slope means we overshot the minimum; therefore, if new slope positive, that alpha must become the new upper bound, forcing the algorithm to search backwards (zoom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "885901ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=1\n",
      "x*=[ 1.39420235e-02 -6.97146574e-03 -4.86065655e-05]\n",
      "f(x*)=3.7786106171351846e-08\n",
      "f(x*_theory)=0\n",
      "==================================================\n",
      "c=10\n",
      "x*=[-6.44639588e-03  3.22344759e-03 -1.03951615e-05]\n",
      "f(x*)=1.7269348598341105e-08\n",
      "f(x*_theory)=0\n",
      "==================================================\n",
      "c=100\n",
      "x*=[ 2.98692859e-03 -1.49431634e-03 -2.23393951e-06]\n",
      "f(x*)=7.962675416221285e-09\n",
      "f(x*_theory)=0\n",
      "==================================================\n",
      "Time to find solutions: 0.4730938000138849s\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "from time import perf_counter\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "class SteepestDescentStrongWolfeConditions:\n",
    "    def __init__(self, epsilon: float = 1e-5, max_iter: int = 10**6):\n",
    "        self.init_alpha = 1.0\n",
    "\n",
    "        if epsilon <= 0:\n",
    "            raise ValueError(\"'epsilon' must be small positive number.\")\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "        if max_iter <= 0 or not isinstance(max_iter, int):\n",
    "            raise ValueError(\"'max_iter' must be positive int number.\")\n",
    "        self._max_iter = max_iter\n",
    "\n",
    "    def solve(self, f: Callable, grad_f: Callable, x0: NDArray) -> NDArray:\n",
    "        \"\"\" \n",
    "        The method to find solution of minimizing function 'f' with gradient 'grad_f' and initial guess 'x0'.\n",
    "        \"\"\"\n",
    "        xk = x0.copy()\n",
    "        for _ in range(0, self._max_iter+1):\n",
    "            grad_f_xk = grad_f(xk)\n",
    "            if np.linalg.norm(grad_f_xk) <= self._epsilon:\n",
    "                break\n",
    "            alpha = self._line_search_alpha_under_strong_wolfe_conditions(f, grad_f, xk)\n",
    "            xk -= alpha * grad_f_xk\n",
    "        else:\n",
    "            warnings.warn(\"You have reached the max iteration limit.\")\n",
    "        return xk\n",
    "        \n",
    "    def _line_search_alpha_under_strong_wolfe_conditions(self, f: Callable, grad_f: Callable, x, c1=1e-3, c2=0.1, max_iter=100):\n",
    "        alpha = self.init_alpha\n",
    "        alpha_min = 0\n",
    "        alpha_max = np.inf\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "\n",
    "            # ! Armijo Condition\n",
    "            if f(x - alpha * grad_f(x)) > f(x) - c1 * alpha * np.dot(-grad_f(x), grad_f(x)):\n",
    "                alpha_max = alpha\n",
    "                alpha = (alpha_min + alpha_max) / 2.0\n",
    "                continue\n",
    "            \n",
    "            # STRONG Wolfe Curvature Condition\n",
    "            if np.abs(np.dot(grad_f(x - alpha * grad_f(x)), -grad_f(x)) <= c2 * np.abs(np.dot(-grad_f(x), grad_f(x)))):\n",
    "                return alpha\n",
    "            \n",
    "            # If slope is positive, we passed the valley floor -> alpha is new upper bound\n",
    "            if np.dot(grad_f(x - alpha * grad_f(x)), -grad_f(x)) >= 0:\n",
    "                alpha_max = alpha\n",
    "            else:\n",
    "                # If slope is negative (and steep), we haven't reached valley -> alpha is lower bound\n",
    "                alpha_min = alpha\n",
    "            \n",
    "            if alpha_max == np.inf:\n",
    "                alpha = alpha * 2.0\n",
    "            else:\n",
    "                alpha = (alpha_min + alpha_max) / 2.0\n",
    "        else:\n",
    "            warnings.warn(\"Line search did not converge (Strong Wolfe).\")\n",
    "        return alpha\n",
    "\n",
    "\n",
    "f_xc = lambda x, c: (x[0]+2*x[1]+x[0]*x[2])*(x[0]+2*x[1]+x[0]*x[2]) + 2 * (x[2]+x[1]*x[1]) * (x[2]+x[1]*x[1]) + c * x[0] * x[0] * x[0] * x[0]\n",
    "grad_f_xc = lambda x, c: np.array([\n",
    "    2 * (x[0] + 2*x[1] + x[0]*x[2]) * (x[2] + 1) + 4 * c * x[0] * x[0] * x[0],\n",
    "    4 * (x[0] + 2*x[1] + x[0]*x[2]) + 8 * x[1] * (x[2] + x[1] * x[1]),\n",
    "    2 * x[0] * (x[0] + 2*x[1] + x[0]*x[2]) + 4 * (x[2] + x[1] * x[1])\n",
    "])\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "t0 = perf_counter()\n",
    "for c in [1, 10, 100]:\n",
    "    f_x = lambda x: f_xc(x, c)\n",
    "    grad_f_x = lambda x: grad_f_xc(x, c)\n",
    "    x0 = np.ones(shape=3)\n",
    "    w_hat = SteepestDescentStrongWolfeConditions().solve(f_x, grad_f_x, x0)\n",
    "\n",
    "    print(f'{c=}')\n",
    "    print(f'x*={w_hat}')\n",
    "    print(f'f(x*)={f_x(w_hat)}')\n",
    "    print(f'f(x*_theory)={f_x([0,0,0])}')\n",
    "    print(50*\"=\")\n",
    "print(f\"Time to find solutions: {(perf_counter()-t0)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d762a6f",
   "metadata": {},
   "source": [
    "So, as we can see, we even got a slight speed boost by slightly changing the alpha search algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a3146",
   "metadata": {},
   "source": [
    "Let's try optimize something even more sophisticated. For example let's maximize likelihood function which is equivalent to minimize cross-entropy ( multiply log-likelihood by $-1/N$) in binary classification problem ($X$ - features, $Y$ - class label, $\\hat{Y}$ - predictions by sigmoid).\n",
    "\n",
    "likelihood function: $ L(w) = \\prod_{i=1}^N \\hat{y_i}^{y_i}  (1-\\hat{y_i})^{1-y_i}$, $w$ - sigmoid weights\n",
    "\n",
    "cross-entropy loss: $ J(w) = -\\frac{1}{N} \\sum_{i=1}^N \\left( y_i log(\\hat{y_i}) + (1-y_i) log(1-\\hat{y_i})\\right) $\n",
    "\n",
    "$\\nabla J(w) = \\frac{1}{N} X^T (\\hat{Y} - Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14a3eeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.96)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(666)\n",
    "\n",
    "N = 50\n",
    "X = np.random.normal(loc=[-1,3], scale=[1,2**0.5], size=(N,2))\n",
    "eps = np.random.normal(loc=0,scale=0.1**0.5,size=N)\n",
    "Y = (3*X[:,0] + X[:,1] + eps > 0).astype(int)\n",
    "\n",
    "\n",
    "X = np.hstack((X, np.ones((N, 1))))\n",
    "\n",
    "sigmoid = lambda w: 1 / (1 + np.exp(-X @ w))\n",
    "cross_entropy = lambda w: -np.mean(Y * np.log(sigmoid(w) + 1e-15) + (1 - Y) * np.log(1 - sigmoid(w) + 1e-15))\n",
    "grad_cross_entropy = lambda w: 1/N * X.T @ (sigmoid(w) - Y)\n",
    "\n",
    "w0 = np.ones(shape=3)\n",
    "w_hat = SteepestDescentStrongWolfeConditions().solve(cross_entropy, grad_cross_entropy, w0)\n",
    "\n",
    "preds = sigmoid(w_hat) >= 0.5\n",
    "acc = np.mean(preds == Y)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e9c12",
   "metadata": {},
   "source": [
    "Great! 96% accuracy!\n",
    "\n",
    "We have just solved a classic classification problem using our hand written solver. Good job!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3972b9",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "In this study, the steepest descent method was implemented and investigated for solving optimization problems.\n",
    "\n",
    "Experiments showed that using a fixed step size ($\\alpha$) is ineffective for functions with complex geometry (using the example of a polynomial with parameter $c=100$), as this leads to slow convergence or the need for manual parameter selection.\n",
    "\n",
    "The implementation of the linear search algorithm using Wolfe's conditions (weak and strict) allowed the step length to be automatically adapted. This provided a significant increase in performance (up to 35 times faster) and stability of the algorithm, since one-dimensional step optimization significantly reduces the number of expensive gradient calculations in multidimensional space.\n",
    "\n",
    "The developed optimizer with strict Wolfe conditions was successfully applied to train a logistic regression model (Cross-Entropy minimization).The algorithm correctly found the optimal weights, ensuring 96% classification accuracy.\n",
    "\n",
    "Thus, the use of adaptive step selection is a critically important component of modern optimization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec7531c",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://optimization.cbe.cornell.edu/index.php?title=Line_search_methods#cite_note-2\n",
    "- https://people.maths.ox.ac.uk/hauser/hauser_lecture2.pdf\n",
    "- https://indrag49.github.io/Numerical-Optimization/line-search-descent-methods.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
